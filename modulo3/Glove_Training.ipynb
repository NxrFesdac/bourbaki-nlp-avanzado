{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Corpus pequeño en español\n",
        "# ---------------------------\n",
        "corpus = [\n",
        "    \"el rey ama a la reina\",\n",
        "    \"la reina ama al rey\",\n",
        "    \"el rey gobierna el reino\",\n",
        "    \"la reina gobierna el reino\",\n",
        "    \"el príncipe es el hijo del rey\",\n",
        "    \"la princesa es la hija de la reina\",\n",
        "    \"parís es la capital de francia\",\n",
        "    \"berlín es la capital de alemania\",\n",
        "    \"roma es la capital de italia\",\n",
        "    \"madrid es la capital de españa\",\n",
        "    \"el dinero está en el banco\",\n",
        "    \"deposité dinero en el banco\",\n",
        "    \"el banco aprobó el préstamo\",\n",
        "    \"trabajo en un banco grande\",\n",
        "    \"me senté en un banco del parque\",\n",
        "    \"el banco de la plaza es de madera\",\n",
        "    \"el banco del parque estaba mojado\",\n",
        "    \"los perros y los gatos son animales\",\n",
        "    \"un perro es un animal leal\",\n",
        "    \"un gato es un animal curioso\",\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizador simple\n",
        "# ---------------------------\n",
        "TOKEN_RE = re.compile(r\"[a-záéíóúüñ]+\", flags=re.IGNORECASE)\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Devuelve lista de tokens (minúsculas), mantiene tildes y ñ.\"\"\"\n",
        "    return TOKEN_RE.findall(text.lower())\n",
        "\n",
        "sentences = [tokenize(s) for s in corpus]\n",
        "\n",
        "# ---------------------------\n",
        "# Vocabulario\n",
        "# ---------------------------\n",
        "vocab = sorted({w for sent in sentences for w in sent})\n",
        "stoi = {w: i for i, w in enumerate(vocab)}\n",
        "itos = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "print(\"Tamaño del vocabulario:\", len(vocab))\n",
        "print(\"Ejemplo de vocabulario:\", vocab[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpMQN0BpbhRY",
        "outputId": "42af5c72-d1e2-4e4d-ebbf-16f604e2b3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 53\n",
            "Ejemplo de vocabulario: ['a', 'al', 'alemania', 'ama', 'animal', 'animales', 'aprobó', 'banco', 'berlín', 'capital', 'curioso', 'de', 'del', 'deposité', 'dinero', 'el', 'en', 'es', 'españa', 'estaba', 'está', 'francia', 'gato', 'gatos', 'gobierna']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Co-ocurrencias (ventana con ponderación por distancia)\n",
        "# ---------------------------\n",
        "window = 4\n",
        "cooc = defaultdict(float)\n",
        "\n",
        "for sent in sentences:\n",
        "    ids = [stoi[w] for w in sent]\n",
        "    for center_idx, wi in enumerate(ids):\n",
        "        start = max(0, center_idx - window)\n",
        "        end   = min(len(ids), center_idx + window + 1)\n",
        "        for ctx_pos in range(start, end):\n",
        "            if ctx_pos == center_idx:\n",
        "                continue\n",
        "            wj = ids[ctx_pos]\n",
        "            dist = abs(ctx_pos - center_idx)\n",
        "            cooc[(wi, wj)] += 1.0 / dist  # palabras más cercanas pesan más\n",
        "\n",
        "cooc_items = list(cooc.items())\n",
        "print(\"Pares de co-ocurrencia:\", len(cooc_items))\n",
        "print(\"Ejemplo (primer par):\", cooc_items[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYCuJXTwd1uL",
        "outputId": "d84b916f-d9b5-452a-b473-809dba20c6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pares de co-ocurrencia: 373\n",
            "Ejemplo (primer par): ((15, 46), 2.8333333333333335)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Hiperparámetros y matrices\n",
        "# ---------------------------\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "V = len(vocab)\n",
        "dim = 30     # dimensión embeddings\n",
        "lr = 0.05\n",
        "epochs = 40\n",
        "\n",
        "x_max = 10.0\n",
        "alpha = 0.75\n",
        "\n",
        "W  = np.random.normal(0, 0.1, size=(V, dim))  # word vectors\n",
        "C  = np.random.normal(0, 0.1, size=(V, dim))  # context vectors\n",
        "bW = np.zeros(V)\n",
        "bC = np.zeros(V)\n",
        "\n",
        "def weight_fn(x):\n",
        "    \"\"\"Función de peso de GloVe\"\"\"\n",
        "    return 1.0 if x >= x_max else (x / x_max) ** alpha"
      ],
      "metadata": {
        "id": "V6OjXbWNd1xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Entrenamiento (SGD)\n",
        "# ---------------------------\n",
        "for epoch in range(epochs):\n",
        "    random.shuffle(cooc_items)\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for (i, j), x in cooc_items:\n",
        "        w = weight_fn(x)\n",
        "        pred = W[i].dot(C[j]) + bW[i] + bC[j]\n",
        "        diff = pred - math.log(x)\n",
        "        total_loss += w * diff * diff\n",
        "\n",
        "        grad = 2 * w * diff\n",
        "\n",
        "        wi_old = W[i].copy()\n",
        "\n",
        "        W[i]  -= lr * grad * C[j]\n",
        "        C[j]  -= lr * grad * wi_old\n",
        "        bW[i] -= lr * grad\n",
        "        bC[j] -= lr * grad\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\"época\", epoch + 1, \"| pérdida promedio\", total_loss / len(cooc_items))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on9DAybqd10A",
        "outputId": "913ab571-b241-43f5-9774-d8a10b6e5249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "época 10 | pérdida promedio 0.05475483062432847\n",
            "época 20 | pérdida promedio 0.02819161635472256\n",
            "época 30 | pérdida promedio 0.01721151996682706\n",
            "época 40 | pérdida promedio 0.01049971095522658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Embedding final y normalización\n",
        "# ---------------------------\n",
        "E = W + C\n",
        "eps = 1e-8\n",
        "norms = np.linalg.norm(E, axis=1, keepdims=True)\n",
        "E = E / (norms + eps)\n",
        "\n",
        "# ---------------------------\n",
        "# Similitud coseno\n",
        "# ---------------------------\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "def most_similar(word, topn=8):\n",
        "    if word not in stoi:\n",
        "        print(f\"'{word}' no está en el vocabulario.\")\n",
        "        return\n",
        "\n",
        "    i = stoi[word]\n",
        "    query = E[i]\n",
        "\n",
        "    scores = []\n",
        "    for j in range(len(E)):\n",
        "        if j == i:\n",
        "            continue\n",
        "        sim = cosine(query, E[j])\n",
        "        scores.append((sim, j))\n",
        "\n",
        "    scores.sort(reverse=True)\n",
        "    for sim, j in scores[:topn]:\n",
        "        print(f\"{itos[j]:15s} {sim:.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Ejemplos\n",
        "# ---------------------------\n",
        "queries = [\"rey\", \"reina\", \"banco\", \"dinero\", \"préstamo\", \"parque\", \"capital\", \"españa\", \"perro\", \"gato\"]\n",
        "for w in queries:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Palabra:\", w)\n",
        "    most_similar(w, topn=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5gHCdvHd12r",
        "outputId": "b2b92ea2-09b7-4d16-aabb-852f7225007c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Palabra: rey\n",
            "el              0.6684\n",
            "al              0.6106\n",
            "gobierna        0.6100\n",
            "madera          0.5981\n",
            "leal            0.5850\n",
            "banco           0.5843\n",
            "francia         0.5343\n",
            "del             0.5302\n",
            "\n",
            "==================================================\n",
            "Palabra: reina\n",
            "la              0.7672\n",
            "es              0.7237\n",
            "plaza           0.6199\n",
            "capital         0.4931\n",
            "gato            0.4479\n",
            "perro           0.4412\n",
            "deposité        0.4209\n",
            "de              0.4100\n",
            "\n",
            "==================================================\n",
            "Palabra: banco\n",
            "el              0.8222\n",
            "gobierna        0.5963\n",
            "rey             0.5843\n",
            "en              0.5076\n",
            "leal            0.4843\n",
            "del             0.4713\n",
            "francia         0.4201\n",
            "reino           0.4127\n",
            "\n",
            "==================================================\n",
            "Palabra: dinero\n",
            "roma            0.3930\n",
            "me              0.3445\n",
            "deposité        0.3264\n",
            "hija            0.3184\n",
            "al              0.2734\n",
            "mojado          0.2728\n",
            "perros          0.2453\n",
            "está            0.2207\n",
            "\n",
            "==================================================\n",
            "Palabra: préstamo\n",
            "madera          0.3489\n",
            "senté           0.3119\n",
            "italia          0.2973\n",
            "reino           0.2866\n",
            "alemania        0.2617\n",
            "mojado          0.2391\n",
            "madrid          0.2241\n",
            "berlín          0.1916\n",
            "\n",
            "==================================================\n",
            "Palabra: parque\n",
            "del             0.7606\n",
            "me              0.3360\n",
            "está            0.2550\n",
            "estaba          0.2496\n",
            "deposité        0.2360\n",
            "curioso         0.2222\n",
            "alemania        0.2191\n",
            "gatos           0.1934\n",
            "\n",
            "==================================================\n",
            "Palabra: capital\n",
            "de              0.9638\n",
            "la              0.8681\n",
            "es              0.7285\n",
            "plaza           0.6901\n",
            "reina           0.4931\n",
            "hija            0.4443\n",
            "gato            0.4155\n",
            "perros          0.3736\n",
            "\n",
            "==================================================\n",
            "Palabra: españa\n",
            "italia          0.6299\n",
            "francia         0.5030\n",
            "leal            0.4719\n",
            "grande          0.4671\n",
            "madera          0.4471\n",
            "reino           0.4230\n",
            "curioso         0.4171\n",
            "al              0.3900\n",
            "\n",
            "==================================================\n",
            "Palabra: perro\n",
            "reina           0.4412\n",
            "gato            0.4355\n",
            "es              0.4276\n",
            "la              0.4124\n",
            "son             0.3751\n",
            "parís           0.3365\n",
            "deposité        0.2300\n",
            "un              0.2294\n",
            "\n",
            "==================================================\n",
            "Palabra: gato\n",
            "es              0.4886\n",
            "la              0.4875\n",
            "un              0.4807\n",
            "reina           0.4479\n",
            "perro           0.4355\n",
            "capital         0.4155\n",
            "plaza           0.3969\n",
            "de              0.3701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3CVtGDJhJMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}