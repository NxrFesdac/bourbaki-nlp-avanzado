{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCB9zHZLGLr66Z9MflnrfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NxrFesdac/bourbaki-nlp-avanzado/blob/main/modulo2/n_grams_text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ9Su6K9chEr",
        "outputId": "e4b2f001-ee0f-4b62-a6f8-266f84dc69ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: 3-gram with Stupid Backoff α=0.4, top-k=30, top-p=0.9, T=0.9\n",
            "Vocabulary size: 144\n",
            "\n",
            "=== Samples ===\n",
            "1. De nuestra muerte, amor, muero de ti, lo sabemos, lo sabemos, lo sabemos, lo ignoran, muero de ambos,\n",
            "2. De nuestra muerte, amor, muero, te muero, te muero, morimos.\n",
            "3. Morimos en el lugar en que estoy solo,\n",
            "4. En el cine y los parques, los que a ti, amor, muero de ambos,\n",
            "5. Muero de ambos,\n",
            "\n",
            "Top continuations for context: muero\n",
            "de              1.000000\n",
            ",               0.120000\n",
            "en              0.040000\n",
            "</s>            0.014314\n",
            "y               0.004771\n",
            "que             0.004135\n",
            "mi              0.003817\n",
            ".               0.003181\n",
            "muero           0.003181\n",
            "ti              0.002863\n",
            "morimos         0.002545\n",
            "amor            0.002227\n",
            "los             0.002227\n",
            "el              0.001590\n",
            "m               0.001590\n"
          ]
        }
      ],
      "source": [
        "import re, random, math\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "random.seed(44)\n",
        "\n",
        "# ------------------ Config ------------------\n",
        "N = 3                 # trigram model\n",
        "ALPHA = 0.4           # backoff factor (Stupid Backoff)\n",
        "TOP_K = 30            # sample from top-k tokens for coherence (None/0 disables)\n",
        "TOP_P = 0.9           # nucleus sampling threshold (None/0/1.0 disables)\n",
        "TEMPERATURE = 0.9     # softens/sharpens distribution (1.0 = off)\n",
        "MAX_LEN = 40          # max tokens to generate\n",
        "NUM_SAMPLES = 5       # how many sentences to sample in demo\n",
        "\n",
        "START, END = \"<s>\", \"</s>\"\n",
        "\n",
        "# ------------------ Toy public-domain-ish corpus ------------------\n",
        "CORPUS = \"\"\"\n",
        "No es que muera de amor, muero de ti.\n",
        "Muero de ti, amor, de amor de ti,\n",
        "de urgencia mía de mi piel de ti,\n",
        "de mi alma, de ti y de mi boca\n",
        "y del insoportable que yo soy sin ti.\n",
        "\n",
        "Muero de ti y de mi, muero de ambos,\n",
        "de nosotros, de ese,\n",
        "desgarrado, partido,\n",
        "me muero, te muero, lo morimos.\n",
        "\n",
        "Morimos en mi cuarto en que estoy solo,\n",
        "en mi cama en que faltas,\n",
        "en la calle donde mi brazo va vacío,\n",
        "en el cine y los parques, los tranvías,\n",
        "los lugares donde mi hombro\n",
        "acostumbra tu cabeza\n",
        "y mi mano tu mano\n",
        "y todo yo te sé como yo mismo.\n",
        "\n",
        "Morimos en el sitio que le he prestado al aire\n",
        "para que estés fuera de mí,\n",
        "y en el lugar en que el aire se acaba\n",
        "cuando te echo mi piel encima\n",
        "y nos conocemos en nosotros,\n",
        "separados del mundo, dichosa, penetrada,\n",
        "y cierto , interminable.\n",
        "\n",
        "Morimos, lo sabemos, lo ignoran, nos morimos\n",
        "entre los dos, ahora, separados,\n",
        "del uno al otro, diariamente,\n",
        "cayéndonos en múltiples estatuas,\n",
        "en gestos que no vemos,\n",
        "en nuestras manos que nos necesitan.\n",
        "\n",
        "Nos morimos, amor, muero en tu vientre\n",
        "que no muerdo ni beso,\n",
        "en tus muslos dulcísimos y vivos,\n",
        "en tu carne sin fin, muero de máscaras,\n",
        "de triángulos oscuros e incesantes.\n",
        "Muero de mi cuerpo y de tu cuerpo,\n",
        "de nuestra muerte ,amor, muero, morimos.\n",
        "En el pozo de amor a todas horas,\n",
        "inconsolable, a gritos,\n",
        "dentro de mi, quiero decir, te llamo,\n",
        "te llaman los que nacen, los que vienen\n",
        "de atrás, de ti, los que a ti llegan.\n",
        "Nos morimos, amor, y nada hacemos\n",
        "sino morirnos más, hora tras hora,\n",
        "y escribirnos y hablarnos y morirnos.\n",
        "\"\"\"\n",
        "\n",
        "# ------------------ Tokenization ------------------\n",
        "def tokenize(text):\n",
        "    # Split into tokens: words or punctuation\n",
        "    # Lowercasing for counts; you can keep case if preferred\n",
        "    return re.findall(r\"[A-Za-z']+|[0-9]+|[^\\w\\s]\", text.lower())\n",
        "\n",
        "def detok(tokens):\n",
        "    # Simple detokenizer with decent punctuation spacing\n",
        "    s = \"\"\n",
        "    for i, t in enumerate(tokens):\n",
        "        if t in {\".\", \",\", \"!\", \"?\", \":\", \";\", \")\", \"]\", \"'\"}:\n",
        "            s += t\n",
        "        elif t in {\"(\", \"[\"}:\n",
        "            if s and not s.endswith(\" \"): s += \" \"\n",
        "            s += t\n",
        "        elif i == 0:\n",
        "            s += t.capitalize()\n",
        "        else:\n",
        "            if s and not s.endswith(\" \"): s += \" \"\n",
        "            s += t\n",
        "    # Ensure ending punctuation\n",
        "    if s and s[-1].isalnum():\n",
        "        s += \".\"\n",
        "    return s\n",
        "\n",
        "# ------------------ N-gram counting ------------------\n",
        "def build_counts(lines, n=N):\n",
        "    counts = [Counter() for _ in range(n+1)]  # counts[k] holds k-gram counts (k>=1)\n",
        "    vocab = set()\n",
        "    for line in lines:\n",
        "        toks = [START]*(n-1) + tokenize(line) + [END]\n",
        "        vocab.update(toks)\n",
        "        for k in range(1, n+1):\n",
        "            for i in range(len(toks)-k+1):\n",
        "                gram = tuple(toks[i:i+k])\n",
        "                counts[k][gram] += 1\n",
        "    # We remove START from sampling vocab; END is allowed as a stop signal\n",
        "    vocab.discard(START)\n",
        "    return counts, sorted(vocab)\n",
        "\n",
        "# ------------------ Stupid Backoff probability (unnormalized) ------------------\n",
        "def sb_score(counts, context, w, alpha=ALPHA):\n",
        "    # Returns an unnormalized \"score\" proportional to probability\n",
        "    # If n-gram unseen, back off to smaller context and multiply by alpha each step.\n",
        "    n = len(counts)-1\n",
        "    for k in range(n, 0, -1):\n",
        "        ctx = tuple(context[-(k-1):]) if k > 1 else tuple()\n",
        "        gram = ctx + (w,)\n",
        "        c = counts[k].get(gram, 0)\n",
        "        if c > 0:\n",
        "            # denom: sum of all next-token counts from this context\n",
        "            if k == 1:\n",
        "                denom = sum(counts[1].values())\n",
        "            else:\n",
        "                denom = 0\n",
        "                for g, v in counts[k].items():\n",
        "                    if g[:-1] == ctx:\n",
        "                        denom += v\n",
        "                if denom == 0:\n",
        "                    denom = 1\n",
        "            steps = n - k\n",
        "            return (alpha**steps) * (c / denom)\n",
        "    return 1e-12\n",
        "\n",
        "# ------------------ Sampling: top-k + top-p + temperature ------------------\n",
        "def top_k_top_p_sample(scores, top_k=TOP_K, top_p=TOP_P, temperature=TEMPERATURE):\n",
        "    \"\"\"\n",
        "    scores: dict[token] -> nonnegative score (unnormalized ok)\n",
        "    Applies temperature, then optional top-k, then optional top-p, then samples.\n",
        "    \"\"\"\n",
        "    items = [(w, s) for w, s in scores.items() if s > 0]\n",
        "    if not items:\n",
        "        items = list(scores.items())\n",
        "    if not items:\n",
        "        raise ValueError(\"No tokens to sample from (scores is empty).\")\n",
        "\n",
        "    # Sort by score descending\n",
        "    items.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Apply temperature (power transform works with your probability-like scores)\n",
        "    if temperature is not None and temperature > 0 and temperature != 1.0:\n",
        "        items = [(w, s ** (1.0 / temperature)) for w, s in items]\n",
        "\n",
        "    # Normalize across all items (needed for correct nucleus cutoff)\n",
        "    total = sum(s for _, s in items)\n",
        "    if total <= 0:\n",
        "        return random.choice([w for w, _ in items])\n",
        "\n",
        "    probs = [(w, s / total) for w, s in items]\n",
        "\n",
        "    # Top-k truncation\n",
        "    if top_k is not None and top_k > 0:\n",
        "        probs = probs[:min(top_k, len(probs))]\n",
        "\n",
        "    # Top-p (nucleus) truncation\n",
        "    if top_p is not None and 0 < top_p < 1.0:\n",
        "        cum = 0.0\n",
        "        nucleus = []\n",
        "        for w, p in probs:\n",
        "            nucleus.append((w, p))\n",
        "            cum += p\n",
        "            if cum >= top_p:\n",
        "                break\n",
        "        probs = nucleus\n",
        "\n",
        "    # Renormalize after truncation, then sample\n",
        "    total_p = sum(p for _, p in probs)\n",
        "    if total_p <= 0:\n",
        "        return random.choice([w for w, _ in probs])\n",
        "\n",
        "    r = random.random()\n",
        "    cum = 0.0\n",
        "    for w, p in probs:\n",
        "        cum += p / total_p\n",
        "        if r < cum:\n",
        "            return w\n",
        "    return probs[-1][0]\n",
        "\n",
        "def generate(counts, vocab, max_len=MAX_LEN, top_k=TOP_K, top_p=TOP_P, temperature=TEMPERATURE):\n",
        "    n = len(counts)-1\n",
        "    context = [START]*(n-1)\n",
        "    out = []\n",
        "    for _ in range(max_len):\n",
        "        scores = {w: sb_score(counts, context, w) for w in vocab if w != START}\n",
        "        scores[END] = sb_score(counts, context, END)\n",
        "\n",
        "        w = top_k_top_p_sample(scores, top_k=top_k, top_p=top_p, temperature=temperature)\n",
        "        if w == END:\n",
        "            break\n",
        "        out.append(w)\n",
        "        context.append(w)\n",
        "    return detok(out)\n",
        "\n",
        "# ------------------ Train + Demo ------------------\n",
        "lines = [l.strip() for l in CORPUS.strip().splitlines() if l.strip()]\n",
        "counts, vocab = build_counts(lines, n=N)\n",
        "\n",
        "print(f\"Model: {N}-gram with Stupid Backoff α={ALPHA}, top-k={TOP_K}, top-p={TOP_P}, T={TEMPERATURE}\")\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "\n",
        "print(\"\\n=== Samples ===\")\n",
        "for i in range(NUM_SAMPLES):\n",
        "    print(f\"{i+1}.\", generate(counts, vocab))\n",
        "\n",
        "# Show a quick conditional sample for intuition\n",
        "def show_next_tokens(context_words, top=15):\n",
        "    ctx = [START]*(N-1) + tokenize(\" \".join(context_words)) if context_words else [START]*(N-1)\n",
        "    scores = {w: sb_score(counts, ctx, w) for w in vocab if w != START}\n",
        "    items = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top]\n",
        "    print(f\"\\nTop continuations for context: {' '.join(context_words) if context_words else '<start>'}\")\n",
        "    for w, s in items:\n",
        "        print(f\"{w:15s} {s:.6f}\")\n",
        "\n",
        "show_next_tokens([\"muero\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M15gmumjkxRm"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}